{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "# General libraries.\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# SK-learn libraries for learning.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# SK-learn library for importing the newsgroup data.\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# SK-learn libraries for feature extraction from text.\n",
    "from sklearn.feature_extraction.text import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training label shape: (2034,)\n",
      "test label shape: (677,)\n",
      "dev label shape: (676,)\n",
      "labels names: ['alt.atheism', 'comp.graphics', 'sci.space', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
    "newsgroups_train = fetch_20newsgroups(subset='train',\n",
    "                                      remove=('headers', 'footers', 'quotes'),\n",
    "                                      categories=categories)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test',\n",
    "                                     remove=('headers', 'footers', 'quotes'),\n",
    "                                     categories=categories)\n",
    "\n",
    "num_test = len(newsgroups_test.target)\n",
    "test_data, test_labels = newsgroups_test.data[num_test/2:], newsgroups_test.target[num_test/2:]\n",
    "dev_data, dev_labels = newsgroups_test.data[:num_test/2], newsgroups_test.target[:num_test/2]\n",
    "train_data, train_labels = newsgroups_train.data, newsgroups_train.target\n",
    "\n",
    "print 'training label shape:', train_labels.shape\n",
    "print 'test label shape:', test_labels.shape\n",
    "print 'dev label shape:', dev_labels.shape\n",
    "print 'labels names:', newsgroups_train.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** BEGIN EXAMPLE 0 *****\n",
      "Training label: 1 (comp.graphics)\n",
      "Text:\n",
      "-----\n",
      "\n",
      "Hi,\n",
      "\n",
      "I've noticed that if you only save a model (with all your mapping planes\n",
      "positioned carefully) to a .3DS file that when you reload it after restarting\n",
      "3DS, they are given a default position and orientation.  But if you save\n",
      "to a .PRJ file their positions/orientation are preserved.  Does anyone\n",
      "know why this information is not stored in the .3DS file?  Nothing is\n",
      "explicitly said in the manual about saving texture rules in the .PRJ file. \n",
      "I'd like to be able to read the texture rule information, does anyone have \n",
      "the format for the .PRJ file?\n",
      "\n",
      "Is the .CEL file format available from somewhere?\n",
      "\n",
      "Rych\n",
      "\n",
      "***** END EXAMPLE 0 *****\n",
      "\n",
      "***** BEGIN EXAMPLE 1 *****\n",
      "Training label: 3 (talk.religion.misc)\n",
      "Text:\n",
      "-----\n",
      "\n",
      "\n",
      "\n",
      "Seems to be, barring evidence to the contrary, that Koresh was simply\n",
      "another deranged fanatic who thought it neccessary to take a whole bunch of\n",
      "folks with him, children and all, to satisfy his delusional mania. Jim\n",
      "Jones, circa 1993.\n",
      "\n",
      "\n",
      "Nope - fruitcakes like Koresh have been demonstrating such evil corruption\n",
      "for centuries.\n",
      "\n",
      "***** END EXAMPLE 1 *****\n",
      "\n",
      "***** BEGIN EXAMPLE 2 *****\n",
      "Training label: 2 (sci.space)\n",
      "Text:\n",
      "-----\n",
      "\n",
      "\n",
      " >In article <1993Apr19.020359.26996@sq.sq.com>, msb@sq.sq.com (Mark Brader) \n",
      "\n",
      "MB>                                                             So the\n",
      "MB> 1970 figure seems unlikely to actually be anything but a perijove.\n",
      "\n",
      "JG>Sorry, _perijoves_...I'm not used to talking this language.\n",
      "\n",
      "Couldn't we just say periapsis or apoapsis?\n",
      "\n",
      " \n",
      "\n",
      "***** END EXAMPLE 2 *****\n",
      "\n",
      "***** BEGIN EXAMPLE 3 *****\n",
      "Training label: 0 (alt.atheism)\n",
      "Text:\n",
      "-----\n",
      "\n",
      "I have a request for those who would like to see Charley Wingate\n",
      "respond to the \"Charley Challenges\" (and judging from my e-mail, there\n",
      "appear to be quite a few of you.)  \n",
      "\n",
      "It is clear that Mr. Wingate intends to continue to post tangential or\n",
      "unrelated articles while ingoring the Challenges themselves.  Between\n",
      "the last two re-postings of the Challenges, I noted perhaps a dozen or\n",
      "more posts by Mr. Wingate, none of which answered a single Challenge.  \n",
      "\n",
      "It seems unmistakable to me that Mr. Wingate hopes that the questions\n",
      "will just go away, and he is doing his level best to change the\n",
      "subject.  Given that this seems a rather common net.theist tactic, I\n",
      "would like to suggest that we impress upon him our desire for answers,\n",
      "in the following manner:\n",
      "\n",
      "1. Ignore any future articles by Mr. Wingate that do not address the\n",
      "Challenges, until he answers them or explictly announces that he\n",
      "refuses to do so.\n",
      "\n",
      "--or--\n",
      "\n",
      "2. If you must respond to one of his articles, include within it\n",
      "something similar to the following:\n",
      "\n",
      "    \"Please answer the questions posed to you in the Charley Challenges.\"\n",
      "\n",
      "Really, I'm not looking to humiliate anyone here, I just want some\n",
      "honest answers.  You wouldn't think that honesty would be too much to\n",
      "ask from a devout Christian, would you?  \n",
      "\n",
      "Nevermind, that was a rhetorical question.\n",
      "\n",
      "***** END EXAMPLE 3 *****\n",
      "\n",
      "***** BEGIN EXAMPLE 4 *****\n",
      "Training label: 2 (sci.space)\n",
      "Text:\n",
      "-----\n",
      "\n",
      "AW&ST  had a brief blurb on a Manned Lunar Exploration confernce\n",
      "May 7th  at Crystal City Virginia, under the auspices of AIAA.\n",
      "\n",
      "Does anyone know more about this?  How much, to attend????\n",
      "\n",
      "Anyone want to go?\n",
      "\n",
      "***** END EXAMPLE 4 *****\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_examples = 5\n",
    "for i in range(num_examples):\n",
    "    example = train_data[i]\n",
    "    label_index = train_labels[i]\n",
    "    label_name = newsgroups_train.target_names[label_index]\n",
    "    \n",
    "    print '***** BEGIN EXAMPLE ' + str(i) + ' *****'\n",
    "    print 'Training label: ' + str(label_index) + ' (' + label_name + ')'\n",
    "    print 'Text:'\n",
    "    print '-----'\n",
    "    \n",
    "    print ''\n",
    "    print example\n",
    "    print ''\n",
    "    print '***** END EXAMPLE ' + str(i) + ' *****'\n",
    "    print ''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) Use CountVectorizer to turn the raw training text into feature vectors. You should use the fit_transform function, which makes 2 passes through the data: first it computes the vocabulary (\"fit\"), second it converts the raw text into feature vectors using the vocabulary (\"transform\").\n",
    "\n",
    "The vectorizer has a lot of options. To get familiar with some of them, write code to answer these questions:\n",
    "\n",
    "a. The output of the transform (also of fit_transform) is a sparse matrix: http://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.sparse.csr_matrix.html. What is the size of the vocabulary? What is the average number of non-zero features per example? What fraction of the entries in the matrix are non-zero? Hint: use \"nnz\" and \"shape\" attributes.\n",
    "\n",
    "b. What are the 0th and last feature strings (in alphabetical order)? Hint: use the vectorizer's get_feature_names function.\n",
    "\n",
    "c. Specify your own vocabulary with 4 words: [\"atheism\", \"graphics\", \"space\", \"religion\"]. Confirm the training vectors are appropriately shaped. Now what's the average number of non-zero features per example?\n",
    "\n",
    "d. Instead of extracting unigram word features, use \"analyzer\" and \"ngram_range\" to extract bigram and trigram character features. What size vocabulary does this yield?\n",
    "\n",
    "e. Use the \"min_df\" argument to prune words that appear in fewer than 10 documents. What size vocabulary does this yield?\n",
    "\n",
    "f. Using the standard CountVectorizer, what fraction of the words in the dev data are missing from the vocabulary? Hint: build a vocabulary for both train and dev and look at the size of the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** (2) a. Output of the transform *****\n",
      "\n",
      "  Size of the vocabulary: 26879\n",
      "  Average non-zero features: 96.7059980334\n",
      "  Non-zero fraction: 0.00359782722696\n",
      "\n",
      "***** (2) b. 0th and last feature strings *****\n",
      "\n",
      "  0th feature string: \n",
      "    3ds able about after all and anyone are available be but carefully cel default does explicitly file for format from given have hi if in information is it know like manual mapping model not nothing noticed only orientation planes position positioned positions preserved prj read reload restarting rule rules rych said save saving somewhere stored texture that the their they this to ve when why with you your\n",
      "  last feature string: \n",
      "    \n",
      "  next to last feature string (last is empty): \n",
      "    _anything_ _behaviors_ _knowledgeable _waving about again all an and any application aquainted around as assertion atheism because becomes becoming being belief believe believer believer_ bit blah by call can carelessly caste certain circular coming contradiction conversely definition do does doesn don equally example except exclusion faith for from gets god good hand_ handedly happen harsh has have having if in inaccessible infer is islam it just justification knowledgable knowledge like limits logical me mean meaningless more much need no not nothing of off offshoot on only other pain penalties produce produces punishment re really religions say saying see sex silly some something speech statement suffering suggests teaches terms than that the then therefore this tiresome to toss true unapplied under what with without word you\n",
      "\n",
      "***** (2) c. 4 word vocabulary *****\n",
      "\n",
      "  Shape of four-word vocabulary vectored training data: (2034, 4)\n",
      "  Average non-zero: 0.268436578171\n",
      "\n",
      "***** (2) d. bigram/trigram character features vocabulary *****\n",
      "\n",
      "  Number of features (word-boundary character bigram/trigram): 28954\n",
      "\n",
      "***** (2) e. mininum document frequency 10 vocabulary *****\n",
      "\n",
      "  Number of features (word-boundary character bigram/trigram): 3064\n",
      "\n",
      "***** (2) f. fraction of missing dev vocabulary *****\n",
      "\n",
      "  Fraction of missing dev features from train vocabulary: 0.247876400345\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#def P2():\n",
    "### STUDENT START ###\n",
    "\n",
    "def vectorize(data):\n",
    "    vectorizer = CountVectorizer()\n",
    "    vectorizer.fit(data)\n",
    "    return vectorizer\n",
    "\n",
    "def select_nonzero_indices(example):\n",
    "    return example.nonzero()[1]\n",
    "\n",
    "def select_features_by_indices(feature_names, indices):\n",
    "    features = [\n",
    "        feature_name\n",
    "        for index, feature_name\n",
    "        in enumerate(feature_names)\n",
    "        if index in indices\n",
    "    ]\n",
    "    return features\n",
    "\n",
    "def select_features_from_example(example, feature_names):\n",
    "    indices = select_nonzero_indices(example)\n",
    "    return select_features_by_indices(feature_names, indices)\n",
    "\n",
    "def select_non_zero_vector(examples):\n",
    "    (rows, columns) = np.shape(examples)\n",
    "    nnz_vector = [\n",
    "        examples[index].nnz\n",
    "        for index\n",
    "        in range(rows)\n",
    "    ]\n",
    "    \n",
    "    return nnz_vector\n",
    "\n",
    "def slice_by_columns(csr_matrix, column_indices):\n",
    "    csc_matrix = csr_matrix.tocsc()\n",
    "    csc_sliced = csc_matrix[:, column_indices]\n",
    "    return csc_sliced.tocsr()\n",
    "\n",
    "def P2():\n",
    "    \n",
    "    def P2a():\n",
    "        vectorizer = vectorize(train_data)\n",
    "        train_vectorized = vectorizer.transform(train_data)\n",
    "\n",
    "        (examples, features) = np.shape(train_vectorized)\n",
    "        non_zero_total = train_vectorized.nnz\n",
    "        non_zero_vector = select_non_zero_vector(train_vectorized)\n",
    "        feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "        print '***** (2) a. Output of the transform *****'\n",
    "        print ''\n",
    "        print '  Size of the vocabulary: ' + str(features)\n",
    "        print '  Average non-zero features: ' + str(np.mean(non_zero_vector))\n",
    "        print '  Non-zero fraction: ' + str((non_zero_total * 1.0) / (examples * features * 1.0))\n",
    "        print ''\n",
    "        \n",
    "        return (vectorizer, train_vectorized, feature_names)\n",
    "    \n",
    "    (vectorizer, train_vectorized, feature_names) = P2a()\n",
    "    \n",
    "    def P2b(vectorizer, train_vectorized, feature_names):\n",
    "        (examples, features) = np.shape(train_vectorized)\n",
    "        zeroeth_example = train_vectorized[0]\n",
    "        last_example = train_vectorized[examples - 1]\n",
    "        next_to_last_example = train_vectorized[examples - 2]\n",
    "\n",
    "        zeroeth_features = select_features_from_example(zeroeth_example, feature_names)\n",
    "        last_features = select_features_from_example(last_example, feature_names)\n",
    "        next_to_last_features = select_features_from_example(next_to_last_example, feature_names)\n",
    "\n",
    "\n",
    "        print '***** (2) b. 0th and last feature strings *****'\n",
    "        print ''\n",
    "        print '  0th feature string: '\n",
    "        print '    ' + ' '.join(zeroeth_features)\n",
    "        print '  last feature string: '\n",
    "        print '    ' + ' '.join(last_features)\n",
    "        print '  next to last feature string (last is empty): '\n",
    "        print '    ' + ' '.join(next_to_last_features)\n",
    "        print ''\n",
    "        \n",
    "    P2b(vectorizer, train_vectorized, feature_names)\n",
    "    \n",
    "    def P2c(feature_names):\n",
    "        four_word_vocabulary = [ 'atheism', 'graphics', 'space', 'religion' ]\n",
    "        four_word_indices = [\n",
    "            index\n",
    "            for index, feature_name\n",
    "            in enumerate(feature_names)\n",
    "            if feature_name in four_word_vocabulary\n",
    "        ]\n",
    "\n",
    "        four_word_train_vectorized = slice_by_columns(train_vectorized, four_word_indices)\n",
    "        four_word_non_zero_vector = select_non_zero_vector(four_word_train_vectorized)\n",
    "\n",
    "\n",
    "        print '***** (2) c. 4 word vocabulary *****'\n",
    "        print ''\n",
    "\n",
    "        print '  Shape of four-word vocabulary vectored training data: ' + \\\n",
    "            str(np.shape(four_word_train_vectorized))\n",
    "        print '  Average non-zero: ' + str(np.mean(four_word_non_zero_vector))\n",
    "        print ''\n",
    "    \n",
    "    P2c(feature_names)\n",
    "    \n",
    "    def P2d():\n",
    "        def vectorize_as_character_bigrams_trigrams(data):\n",
    "            vectorizer = CountVectorizer(analyzer = 'char_wb', ngram_range = (2, 3))\n",
    "            vectorizer.fit(data)\n",
    "            return vectorizer\n",
    "\n",
    "        vectorizer = vectorize_as_character_bigrams_trigrams(train_data)\n",
    "        train_char_bigram_trigram_vectorized = vectorizer.transform(train_data)\n",
    "        (examples, features) = np.shape(train_char_bigram_trigram_vectorized)\n",
    "\n",
    "        print '***** (2) d. bigram/trigram character features vocabulary *****'\n",
    "        print ''\n",
    "        print '  Number of features (word-boundary character bigram/trigram): ' + str(features)\n",
    "        print ''\n",
    "    \n",
    "    P2d()\n",
    "    \n",
    "    def P2e():\n",
    "        def vectorize_with_document_frequency_pruning(data, min_df = 10):\n",
    "            vectorizer = CountVectorizer(min_df = min_df)\n",
    "            vectorizer.fit(data)\n",
    "            return vectorizer\n",
    "\n",
    "        vectorizer = vectorize_with_document_frequency_pruning(train_data, 10)\n",
    "        train_vectorized_pruned_min_df_10 = vectorizer.transform(train_data)\n",
    "        (examples, features) = np.shape(train_vectorized_pruned_min_df_10)\n",
    "\n",
    "        print '***** (2) e. mininum document frequency 10 vocabulary *****'\n",
    "        print ''\n",
    "        print '  Number of features (word-boundary character bigram/trigram): ' + str(features)\n",
    "        print ''\n",
    "    \n",
    "    P2e()\n",
    "    \n",
    "    def P2f():\n",
    "        train_feature_names = vectorize(train_data).get_feature_names()\n",
    "        dev_feature_names = vectorize(dev_data).get_feature_names()\n",
    "        feature_names_intersection = set(train_feature_names).intersection(dev_feature_names)\n",
    "\n",
    "        missing_fraction = 1.0 - ((1.0 * len(feature_names_intersection)) / (1.0 * len(dev_feature_names)))\n",
    "\n",
    "        print '***** (2) f. fraction of missing dev vocabulary *****'\n",
    "        print ''\n",
    "        print '  Fraction of missing dev features from train vocabulary: ' + str(missing_fraction)\n",
    "        print ''\n",
    "        \n",
    "    P2f()\n",
    "    \n",
    "    \n",
    "P2()\n",
    "\n",
    "### STUDENT END ###\n",
    "#P2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** (2) c. 4 word vocabulary *****\n",
      "\n",
      "  Shape of four-word vocabulary vectored training data: (2034, 4)\n",
      "  Average non-zero: 0.268436578171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "four_word_vocabulary = [ 'atheism', 'graphics', 'space', 'religion' ]\n",
    "four_word_indices = [\n",
    "    index\n",
    "    for index, feature_name\n",
    "    in enumerate(feature_names)\n",
    "    if feature_name in four_word_vocabulary\n",
    "]\n",
    "\n",
    "four_word_train_vectorized = slice_by_columns(train_vectorized, four_word_indices)\n",
    "four_word_non_zero_vector = select_non_zero_vector(four_word_train_vectorized)\n",
    "\n",
    "\n",
    "print '***** (2) c. 4 word vocabulary *****'\n",
    "print ''\n",
    "\n",
    "print '  Shape of four-word vocabulary vectored training data: ' + \\\n",
    "    str(np.shape(four_word_train_vectorized))\n",
    "print '  Average non-zero: ' + str(np.mean(four_word_non_zero_vector))\n",
    "print ''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** (2) d. bigram/trigram character features vocabulary *****\n",
      "\n",
      "  Number of features (word-boundary character bigram/trigram): 28954\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def vectorize_as_character_bigrams_trigrams(data):\n",
    "    vectorizer = CountVectorizer(analyzer = 'char_wb', ngram_range = (2, 3))\n",
    "    vectorizer.fit(data)\n",
    "    return vectorizer\n",
    "\n",
    "vectorizer = vectorize_as_character_bigrams_trigrams(train_data)\n",
    "train_char_bigram_trigram_vectorized = vectorizer.transform(train_data)\n",
    "(examples, features) = np.shape(train_char_bigram_trigram_vectorized)\n",
    "\n",
    "print '***** (2) d. bigram/trigram character features vocabulary *****'\n",
    "print ''\n",
    "print '  Number of features (word-boundary character bigram/trigram): ' + str(features)\n",
    "print ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** (2) e. mininum document frequency 10 vocabulary *****\n",
      "\n",
      "  Number of features (word-boundary character bigram/trigram): 3064\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def vectorize_with_document_frequency_pruning(data, min_df = 10):\n",
    "    vectorizer = CountVectorizer(min_df = min_df)\n",
    "    vectorizer.fit(data)\n",
    "    return vectorizer\n",
    "\n",
    "vectorizer = vectorize_with_document_frequency_pruning(train_data, 10)\n",
    "train_vectorized_pruned_min_df_10 = vectorizer.transform(train_data)\n",
    "(examples, features) = np.shape(train_vectorized_pruned_min_df_10)\n",
    "\n",
    "print '***** (2) e. mininum document frequency 10 vocabulary *****'\n",
    "print ''\n",
    "print '  Number of features (word-boundary character bigram/trigram): ' + str(features)\n",
    "print ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** (2) f. fraction of missing dev vocabulary *****\n",
      "\n",
      "  Fraction of missing dev features from train vocabulary: 0.247876400345\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "16246"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_feature_names = vectorize(train_data).get_feature_names()\n",
    "dev_feature_names = vectorize(dev_data).get_feature_names()\n",
    "feature_names_intersection = set(train_feature_names).intersection(dev_feature_names)\n",
    "\n",
    "missing_fraction = 1.0 - ((1.0 * len(feature_names_intersection)) / (1.0 * len(dev_feature_names)))\n",
    "\n",
    "print '***** (2) f. fraction of missing dev vocabulary *****'\n",
    "print ''\n",
    "print '  Fraction of missing dev features from train vocabulary: ' + str(missing_fraction)\n",
    "print ''\n",
    "\n",
    "len(dev_feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 jobs       | elapsed:    0.1s\n",
      "[Parallel(n_jobs=1)]: Done  27 out of  27 | elapsed:    2.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "\n",
    "vectorizer = vectorize(train_data)\n",
    "train_vectorized = vectorizer.transform(train_data)\n",
    "dev_vectorized = vectorizer.transform(dev_data)\n",
    "\n",
    "def best_nearest_neighbors(train_vectorized, train_labels, n_neighbors = range(1, 10)):\n",
    "    \n",
    "    hyperparameters = { 'n_neighbors' : n_neighbors } \n",
    "    nearest_neighbors = KNeighborsClassifier()\n",
    "    \n",
    "    grid_search = GridSearchCV(nearest_neighbors, hyperparameters, verbose = 1, scoring = 'f1')\n",
    "    grid_search.fit(train_vectorized, train_labels)\n",
    "    \n",
    "    return grid_search.best_estimator_\n",
    "\n",
    "nearest_neighbors = best_nearest_neighbors(train_vectorized, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** (3) a. k nearest neighbors *****\n",
      "\n",
      "  Best k: 9\n",
      "  F1 score over dev data: 0.43656661762\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_n_neighbors = nearest_neighbors.get_params()['n_neighbors']\n",
    "dev_predictions = nearest_neighbors.predict(dev_vectorized)\n",
    "knn_f1_score = metrics.f1_score(dev_labels, dev_predictions)\n",
    "\n",
    "print '***** (3) a. k nearest neighbors *****'\n",
    "print ''\n",
    "print '  Best k: ' + str(best_n_neighbors)\n",
    "print '  F1 score over dev data: ' + str(knn_f1_score)\n",
    "print ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_neighbors=5, p=2, weights='uniform')"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nearest_5_neighbors = KNeighborsClassifier(n_neighbors = 5)\n",
    "nearest_5_neighbors.fit(train_vectorized, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Mean cosine distance to nearest neighbors: 0.497190184605\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def cosine_distance(a, b):\n",
    "    return pairwise_distances(a, b, metric='cosine')[0][0]\n",
    "\n",
    "def select_nearest_neighbors(model, examples, example):\n",
    "    neighbor_indices = model.kneighbors(example)[1][0]\n",
    "    return examples[neighbor_indices]\n",
    "\n",
    "def mean_neighbor_cosine_distance(model, examples):\n",
    "    (n_examples, features) = np.shape(examples)\n",
    "    cosine_distances = [ ]\n",
    "    \n",
    "    for example_index in range(n_examples):\n",
    "        example = examples[example_index]\n",
    "        neighbors = select_nearest_neighbors(model, examples, example)\n",
    "        (n_neighbors, features) = np.shape(neighbors)\n",
    "\n",
    "        example_cosine_distances = [\n",
    "            cosine_distance(neighbors[index], example)\n",
    "            for index in range(n_neighbors)\n",
    "            if index != example_index\n",
    "        ]\n",
    "\n",
    "        cosine_distances += example_cosine_distances\n",
    "    \n",
    "    return np.mean(cosine_distances)\n",
    "                                  \n",
    "mean_distance = mean_neighbor_cosine_distance(nearest_5_neighbors, train_vectorized)\n",
    "print ' Mean cosine distance to nearest neighbors: ' + str(mean_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** (3) b. Naive Bayes and Logistic Regression *****\n",
      "  Naive Bayes best alpha: 0.01 of [0.0, 0.0001, 0.001, 0.01, 0.1, 0.5, 1.0]\n",
      "  Naive Bayes F1 score over dev data: 0.775166321854\n",
      "\n",
      "  Logistic Regression best C: 0.5 of [0.0001, 0.001, 0.01, 0.1, 0.5, 1.0, 5.0, 10.0, 100.0]\n",
      "  Logistic Regression F1 score over dev data: 0.708473977649\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python2.7/site-packages/sklearn/naive_bayes.py:435: RuntimeWarning: divide by zero encountered in log\n",
      "  self.feature_log_prob_ = (np.log(smoothed_fc)\n"
     ]
    }
   ],
   "source": [
    "def best_naive_bayes(train_data, train_labels, hyperparameters, verbose = 0):\n",
    "    naive_bayes = MultinomialNB()\n",
    "    grid_search = GridSearchCV(naive_bayes, hyperparameters, verbose = verbose, scoring = 'f1')\n",
    "    grid_search.fit(train_data, train_labels)\n",
    "    \n",
    "    return grid_search.best_estimator_\n",
    "    \n",
    "naive_bayes_hyperparameters = {'alpha': [0.0, 0.0001, 0.001, 0.01, 0.1, 0.5, 1.0]}\n",
    "\n",
    "naive_bayes = best_naive_bayes(\n",
    "    train_vectorized, train_labels, naive_bayes_hyperparameters\n",
    ")\n",
    "\n",
    "best_naive_bayes_alpha = naive_bayes.get_params()['alpha']\n",
    "dev_predictions = naive_bayes.predict(dev_vectorized)\n",
    "naive_bayes_f1_score = metrics.f1_score(dev_labels, dev_predictions)\n",
    "\n",
    "def best_logistic_regression(train_data, train_labels, hyperparameters, verbose = 0):\n",
    "    logistic_regression = LogisticRegression()\n",
    "    grid_search = GridSearchCV(logistic_regression, hyperparameters, verbose = verbose, scoring = 'f1')\n",
    "    \n",
    "    grid_search.fit(train_data, train_labels)\n",
    "    \n",
    "    return grid_search.best_estimator_\n",
    "\n",
    "logistic_regression_hyperparameters = {'C': [0.0001, 0.001, 0.01, 0.1, 0.5, 1.0, 5.0, 10.0, 100.0]}\n",
    "\n",
    "logistic_regression = best_logistic_regression(\n",
    "    train_vectorized, train_labels, logistic_regression_hyperparameters\n",
    ")\n",
    "best_logistic_regression_C = logistic_regression.get_params()['C']\n",
    "\n",
    "dev_predictions = logistic_regression.predict(dev_vectorized)\n",
    "logistic_regression_f1_score = metrics.f1_score(dev_labels, dev_predictions)\n",
    "\n",
    "print '***** (3) b. Naive Bayes and Logistic Regression *****'\n",
    "print '  Naive Bayes best alpha: ' + str(best_naive_bayes_alpha) + \\\n",
    "  ' of ' + str(naive_bayes_hyperparameters['alpha'])\n",
    "print '  Naive Bayes F1 score over dev data: ' + str(naive_bayes_f1_score)\n",
    "print ''\n",
    "print '  Logistic Regression best C: ' + str(best_logistic_regression_C) + \\\n",
    "  ' of ' + str(logistic_regression_hyperparameters['C'])\n",
    "print '  Logistic Regression F1 score over dev data: ' + str(logistic_regression_f1_score)\n",
    "print ''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** (3) b. Naive Bayes and Logistic Regression *****\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": [
       "iVBORw0KGgoAAAANSUhEUgAAAY0AAAEPCAYAAAC+35gCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\n",
       "AAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xuc3dO9//HXO+4kGkqJiEY1Wuoet6IV5aijBD11Ob82\n",
       "KHWc0tKbX0VPK72cFlVFW9W6Faeiqqg7EaJUCRES4l5R0ojeXA+V8Dl/rDX2zpjMfPfM3vPdl/fz\n",
       "8ZjHfPd3f/f+fmbE/sxa67PWUkRgZmZWxJCyAzAzs9bhpGFmZoU5aZiZWWFOGmZmVpiThpmZFeak\n",
       "YWZmhTUsaUgaJekWSQ9KekDSUfn8JEnPSJqZv/616jUTJT0m6WFJu1adHytpdn7utEbFbGZmvVOj\n",
       "5mlIWhNYMyLukzQUmAHsDewHvBQRp3S7fkPgImArYCRwEzAmIkLSdOBzETFd0rXA6RFxfUMCNzOz\n",
       "JWpYSyMino2I+/Lxy8BDpGQAoB5eshcwOSIWRsRc4HFgG0kjgGERMT1fdwEp+ZiZ2SAblDENSaOB\n",
       "zYE786nPS7pf0jmShudzawHPVL3sGVKS6X5+HpXkY2Zmg6jhSSN3TV0KHJ1bHD8F1gU2A+YDP2h0\n",
       "DGZmVh9LN/LNJS0D/Ab4n4i4AiAinqt6/mzgqvxwHjCq6uVrk1oY8/Jx9fl5PdzLi2iZmfVDRPQ0\n",
       "ZNCjhiUNSQLOAeZExKlV50dExPz8cB9gdj6+ErhI0imk7qcxwPQ8EP6ipG2A6cAE4PSe7lnLD97O\n",
       "JE2KiEllx9EM/Luo8O+iwr+Lilr/4G5kS2N74FPALEkz87njgH+XtBkQwJPA4QARMUfSJcAcYBFw\n",
       "RFRKu44AfgGsAFzryikzs3I0LGlExO30PGZyXS+v+S7w3R7OzwA2rl90ZmbWH54R3p6mlR1AE5lW\n",
       "dgBNZFrZATSRaWUH0KoaNrlvsEkKj2mYmdWm1s9OtzTMzKwwJw0zMyvMScPMzApz0jAzs8KcNMzM\n",
       "rDAnDTMzK8xJw8zMCnPSMDOzwpw0zMysMCcNMzMrzEnDzMwKc9IwM7PCnDTMzKwwJw0zMyvMScPM\n",
       "zApz0jAzs8KcNMzMrDAnDTMzK8xJw8zMCnPSMDOzwpw0zMysMCcNMzMrzEnDzMwKc9IwM7PCnDTM\n",
       "zKwwJw0zMyvMScPMzApz0jAzs8KWLjsAMzMbXBJDgA8Ce9f6Wrc0zMw6gMSyErtJ/Az4M3Am8Gqt\n",
       "79Nn0pC0n6SV8/HXJV0uaYuaIzYzs0ElMVRiX4mLgAXAN4DHgR0i2DiCb9T8nhHRx001OyI2lrQD\n",
       "8B3gZODrEbFN7T9C40iKiFDZcZiZlUlidWA8sA/wYeAPwOXAbyOY//bra/vsLDKm8Ub+vgdwVkRc\n",
       "LenbRW9gZmaNJfFuUpLYB9gMuBG4CPhUBM/X9V4FWhrXAPOAfwE2B14D7oqITesZyEC5pWFmnUJC\n",
       "wAeoJIpRwFWkFsVNEcXHKmr97CwyEL4vcAOwa0Q8D6wCHFMgkFGSbpH0oKQHJB2Vz68qaYqkRyXd\n",
       "KGl41WsmSnpM0sOSdq06P1bS7PzcaUV/ODOzdiExRGI7iZOAR4FrgdWALwEjIjgkgqtqSRj9iqNA\n",
       "S+PCiJjQ17keXrcmsGZE3CdpKDCDVN71aeCvEXGSpK8Cq0TEsZI2JDWntgJGAjcBYyIiJE0HPhcR\n",
       "0yVdC5weEdd3u59bGmbWViSWBXYitSb2Av5Oak1cDtwbQe8f4IXuUf8xjY263WBpYGxfL4qIZ4Fn\n",
       "8/HLkh4iJYPxwI75svOBacCxpF/I5IhYCMyV9DiwjaSngGERMT2/5gJS8lksaZiZtQOJocBupESx\n",
       "O/AwKUnsGMGjZcYGvSQNSccBE4EVJL1U9dRC4Oe13ETSaNJ4yF3AGhGxID+1AFgjH68F3Fn1smdI\n",
       "SWZhPu4yL583M2sLEqsBe5ISxTjSZ+HlwDER/LnE0N5miUkjIr4LfFfSCRFxbH9vkLumfgMcHREv\n",
       "SZVWUO56GnDzqupek6oeTouIafV6bzOzepJYh9Rrsg+wBalL/hLgoAj+0bj7ahwpMfVLn91Tebxh\n",
       "JPDu6usj4ncFgluGlDAujIgr8ukFktaMiGcljQCey+fnkSoAuqxNamHMy8fV5+ctIdZJfcVkZlaG\n",
       "XPG0ISlJ7A2MBq4GTgVubPQAdpf8x/S0Slw6vpbX95k0JJ0I7A/MoTJnA6DXpKHUpDgHmBMRp1Y9\n",
       "dSVwEHBi/n5F1fmLJJ1C6n4aA0zPrZEXJW0DTAcmAKcX+NnMzEqV13jamkpp7ArkbifgtggWlRhe\n",
       "vxSpnnoU2Dgi/lnTG6cZ5L8DZsFbI/wTSR/8lwDrAHOB/XIpb9c4yiHAIlJ31g35/FjgF6Rf+LUR\n",
       "cVQP93P1lJmVTmIZUvdPV8XTC1QqnmbUo+Kpnmr97CySNK4jfbC/1OuFJXPSMLOySKwEfJSUKD4G\n",
       "PEZOFBE8UmZsfWlEye2rwH2SpgJdrY3o6a99M7NOIfFOKhVPO5GqQy8Hjo3oedy1HRRJGlfmr64m\n",
       "iaqOzcw6hsQoKhVPY4GpwKXAwY2seGomfXZPAUhaEVgnIh5ufEj94+4pM2sEiQ2oDGS/h1TxdDmp\n",
       "4ul/y4ytHhoxpjEe+D6wXESMlrQ58M2IGD+wUOvLScPM6iFXPG1FpTR2KKnK83LgdxEsLDG8umtE\n",
       "0rgX+AhwS0Rsns89EBEb9frCQeakYWb9lSuedqRS8fQSlYqne5qt4qmeGjEQvjAinq+eyQ28WXNk\n",
       "ZmZNRGJFFq94eoKUJHaJoGm74stWJGk8KOmTwNKSxgBHAXc0Niwzs/qTWJVU8bQ3sDNwNylRHBex\n",
       "2Bp3tgRFuqdWAr4GdO1vcQPw7Yh4rcGx1cTdU2bWE4m1qVQ8bUWqeLocuCaCv5UZWzOo+5hGq3DS\n",
       "MLMuEu+nUvG0HnANlYqnV8qMrdnULWlIOi0ijpZ0VQ9Ph6unzKxZ5MUAt6SSKFamUvF0a7tVPNVT\n",
       "PQfCL8jff9DDc+3RPDGzlpUrnj5MpTT2FVKSOBi4O8IFO41QZExjKPBqRLyRHy8FLB8RTdXEc0vD\n",
       "rP3liqddSYliD+CPVNZ4eqjM2FpVI+Zp3AXsHBEv58fDgBsiYrsBRVpnThpm7UliFVKC2IdU8TSD\n",
       "lCiuiODpMmNrB42Yp7FcV8IAyLvvrdiv6MzMCpAYSepy2hvYBriFlCgOc8VTuYokjVckjY2IGQCS\n",
       "toTB2WHKzDqHxPuoDGSPAa4FzgT2dsVT8yjSPbUVcDEwP58aAewfEfc0OLaauHvKrLXkiqexVBLF\n",
       "cBaveHq9xPA6RkPmaUhaFngfqWrqkYhouvI1Jw2z5iexNPAhKhVPr1FZ42m6K54GXz3naewcEVMl\n",
       "/RspWXS9aQBExGUDDbaenDTMmpPECixe8fQUlUQxp50XA2wF9RwI/zBpuv2e9Dwvo6mShpk1D4nh\n",
       "VCqedgHuJXU9HR/BU2XGZgPTW9Lo2oXq7Ii4fTCCMbPWJbEWaVnxfYBtgWmk1sThEfy1xNCsjnrr\n",
       "nro/IjaVNLNrH41m5u4ps8EnMYbKQPb7SRVPlwPXR/Byb6+15lDP7qk5kh4DRkqa3e25iIhN+hWh\n",
       "mbWsXPG0BZVVY98J/BaYBNziiqf212v1lKQ1SUuhj6cyEA5ARMxtaGQ1ckvDrDFyxdMOVCqeXqcy\n",
       "kH2XK55aW91aGpKmRsTOkm6ICA9cmXUQieWBfyElij2Bp0lJ4mPAg6546ly9dU+NkLQ9MF7SxaSW\n",
       "xlv/UCLi3kYHZ2aDR+IdpKSwD6lE9j5SovhWBHNLDM2aSG8D4fsChwLbA2+b/R0ROzU2tNq4e8qs\n",
       "dhIjqFQ8fRC4lVQae2UEfykzNhscjVjl9hsR8a0BR9ZgThpmxUi8l0rF0wbAdVQqnl4qMzYbfI1I\n",
       "GkOATwLrRsS3JK0DrBkR0wcWan05aZj1LFc8bUYlUaxOqni6nFTx9M8Sw7OSNSJpnAm8CXwkIt4v\n",
       "aVXgxojYcmCh1peThlmFxFKkiqeu0tg3qFQ83RnBGyWGZ02kEftpbBMRm0uaCRARf5e0TL8jNLOG\n",
       "yBVPu1CpeJpHShJ7Ag+44snqoUjSeD1v8QqApNXBddlmzSBXPO1OpeJpFilRfCeCJ8uMzdpTkaTx\n",
       "I9I/wndJ+i7wCeC/GhqVmS2RxBpUKp62B24j/T/6uQieKzM2a39F99PYgLQ3L8DUiGi6Ddw9pmHt\n",
       "TGI9KgPZHyBVPF0BXBfBi2XGZq2tEWMaAMtRWUZk2ZqjMrOa5IqnTakkijVIFU/fAW52xZOVpUj1\n",
       "1NHAYaT9M0SqxjgrIk5vfHjFuaVhrS5XPG1HJVEElYqnP7jiyRqhESW3s4FtI+KV/Hgl4M6I2LhA\n",
       "MOeSliV4rut6SZOAz8Bbs02Pi4jr8nMTgUNI5YFHRcSN+fxY4BfA8sC1EXF0D/dy0rCWI7EclYqn\n",
       "8cB8KoliliuerNFq/ewcUvC6N5dw3JfzgN26nQvglIjYPH91JYwNgf2BDfNrzpDU9YP8FDg0IsYA\n",
       "YyR1f0+zliGxssQBEr8CFgATgYeAbSPYNIJJEdzvhGHNqMiYxnnAXZKqu6fOLfLmEXGbpNE9PNVT\n",
       "VtsLmBwRC4G5kh4HtpH0FDCsagb6BTmG64vEYNYMcsXTeFKLYgfgdlJr4qgIFpQZm1kt+kwaEXGK\n",
       "pFtJ/9ADODgiZg7wvp+XdCBpIcQvR8TzwFrAnVXXPAOMBBbm4y7z8nmzpicxFjgJGEvam+YC4ABX\n",
       "PFmr6rN7StK2wGMRcVoe/H5C0jYDuOdPgXVJa+HMB34wgPcya0oSwyV+DFwDTAbWjGD/CC52wrBW\n",
       "VqR76kygeo/wV3o4V1hEvDX5SNLZwFX54TxgVNWla5NaGPPycfX5eT29dx5k7zItIqb1J0az/sql\n",
       "sp8itS5+C2wYwd/LjcqsQtI4YFx/X19onkZUlVhFxBvVy4rUStKIiJifH+4DdO0/fiVwkaRTSN1P\n",
       "Y4DpERGSXsytm+nABKDHct+ImNTfuMwGSuIDwBnAMGDvCO4qOSSzt8l/TE/reizp+FpeXyRpPCnp\n",
       "KFK3koDPAn8s8uaSJgM7AqtJeho4HhgnaTPS+MiTwOEAETFH0iXAHGARcERVsjqCVHK7Aqnk1oPg\n",
       "1jQkhpL+bR8MTALO9JwKa1dF5mmsQfrLvmunvqnA0dXdTM3A8zRssOWuqI8DpwK3AMe4EspaTd0n\n",
       "97UKJw0bTHn3ux+TxuGOiODWkkMy65dGTe4zM0BiBYlvksrDbwI2c8KwTlJ0wUKzjiexO2mrgJnA\n",
       "5hE8XXJIZoOu15aGpCGS9husYMyakcQ6EpcBp5G6oj7hhGGdqtekERFvAl8dpFjMmorEshJfJbUs\n",
       "7gM2juCGksMyK1WR7qkpkr4C/Io0sQ9Ie4U3LCqzkknsBPyEVBa+dQRPlBySWVMoUnI7F9622mZE\n",
       "xHsaFVR/uHrK6kFiBHAyaa21LwBXeLVZa2d137kvIkYPKCKzFiCxNGkS6deBc0jLf7zS+6vMOk+f\n",
       "SUPSsqRZ4B8mtThuBc7MS5ibtTyJbUkrHjwP7BjBnJJDMmtaRbqnziEll/NJy4hMABZFxGcaH15x\n",
       "7p6yWkm8EziBtLvkV4DJ7oqyTlP37ilgq4jYpOrxVEmzag/NrDlIDCFtK/zfwMXABhG8UG5UZq2h\n",
       "SNJYJOm9EfE4gKT1SAsKmrUcic1JK9EC7BbBQDcUM+soRZLGMcDNkp7Mj0cDn25YRGYNIPEO4FvA\n",
       "AcDXgHMjatrv3szoJWlI2jcifk2qU18feF9+6pGIeG0wgjMbqLwS7f8Dvk/aRe8DEfy13KjMWtcS\n",
       "B8IlzYyIzbu+D3JcNfNAuHUnsQGpK2o4afmPP5QcklnTqedA+N8kTQHWlXRVt+ciIsb3K0KzBpNY\n",
       "iTTf4lBSl9RPIzwOZ1YPvSWN3YEtgAtJM2SrM5HLEq3p5K6ovUmbIt0GbBLB/N5fZWa1KDJP413N\n",
       "tktfT9w91dkk3kNatnxd4MgIbik5JLOWUPdNmFohYVjnklhe4hvAdOB3pE2RnDDMGsSbMFnLktiN\n",
       "1LqYDWwRwZ9KDsms7TlpWMuRGAX8ENgc+HwE15YcklnH6LN7StL3Ja0saRlJUyX9VdKEwQjOrJrE\n",
       "MhLHkDZFegDYyAnDbHD1mTSAXSPiRWAPYC6wHmmWuNmgkdiRlCx2BraNYFIEr5YcllnHKdI91XXN\n",
       "HsClEfGCJJfc2qCQWIM0m3sn0qZIl3klWrPyFGlpXCXpYWAsaYXbdwFeRsQaSmIpiSNJ3VDPklai\n",
       "/Y0Thlm5+pynASBpVeCFiHhD0krAsIh4tuHR1cDzNNqHxDak5T9eJi3/8WDJIZm1rbrP08hJ4kjg\n",
       "zHxqLWDL/oVntmQSq0r8DLiCNKt7nBOGWXMp0j11HvA6sF1+/GfS5jVmdSExROIQYA7p39oGEVzo\n",
       "riiz5lNkIHy9iNhP0gEAEfGK5F4gqw+JTUldUUsDu0dwb8khmVkvirQ0/ilpha4Heee+fzYuJOsE\n",
       "EitL/BCYQtp//oNOGGbNr0jSmARcD6wt6SLgZuCrjQzK2peEJA4AHgJWJm2K9HPvomfWGnrtnpI0\n",
       "BFgF+Ddg23z66Ij4S6MDs/Yj8X7gJ8BqwH4R/L7kkMysRkWWRp8REWMHKZ5+c8lt85JYEfgv4D+A\n",
       "7wA/9qZIZs2h7iW3wBRJX5E0StKqXV8DiNE6iMR4UlXUuqRNkU51wjBrXUVaGnPpYae+iFi3QTH1\n",
       "i1sazUViXeB0YAxpU6SpJYdkZj2o5x7hAETE6AFFZB1FYjnSgpZfJG0T/IkIV9uZtYsi3VNI2kjS\n",
       "fpIO7Poq+LpzJS2QNLvq3KqSpkh6VNKNkoZXPTdR0mOSHpa0a9X5sZJm5+dOq+UHtMEj8S+kDZG2\n",
       "BMZG8D0nDLP2UmQZkUmk3dF+TFpp9CRgfMH3Pw/Yrdu5Y4EpEbE+MDU/RtKGwP7Ahvk1Z6gyi/Cn\n",
       "wKERMQYYI6n7e1qJJEZK/Ar4GfClCPaOYG7JYZlZAxRpaXwC2AWYHxGfBjYFhvf+kiQibgP+0e30\n",
       "eNJkLvL3vfPxXsDkiFgYEXOBx4FtJI0gLZA4PV93QdVrrER5U6QvAfcDj5I2Rbq65LDMrIGKLCPy\n",
       "al7ddpGkdwDPAaMGcM81ImJBPl4ArJGP1wLurLruGWAksDAfd5mXz1uJJD5EWv5jPrBdBI+WHJKZ\n",
       "DYIiSeNuSasAZwH3AK8Ad9Tj5hER3tCptUi8i9RFuQtpsPtSLyxo1jmKVE8dkQ/PlHQDqato1gDu\n",
       "uUDSmhHxbO56ei6fn8fiLZi1SS2Mefm4+vy8nt44j790mRYR0wYQp1WRWIo0Oe+bpC7CDSJ4qdyo\n",
       "zKxWksYB4/r9+gLzNHak53kavyt0A2k0cFVEbJwfnwT8LSJOlHQsMDwijs0D4RcBW5O6n24C3ptb\n",
       "I3cBRwHTgWuA0yPi+m738TyNBpHYitQV9RppU6TZfbzEzFpErZ+dRZLG1VSSxvKkD/UZEfGRAsFM\n",
       "BnYkrTW0APgG8FvgEmAdYC6wX0Q8n68/DjgEWERa4+qGfH4s8AtgBeDaiDiqh3s5adSZxCqkvVP2\n",
       "IVW5XeCuKLP2Uvek0cMNRgGnRcTHaw2ukZw06kdCwIHAicBlwNci3lYFZ2ZtoO4zwnvwDLBBP15n\n",
       "LUBiY1JX1PLAnhHcXXJIZtZE+kwakn5U9XAIsBkwo2ERWSkkhpH2TplA6kY8K4I3Sg3KzJpOkZZG\n",
       "dYJYRJqAd3uD4rFBlrui9gVOIe2it1HEWxVtZmaLqXlMo1l5TKN2EuuTlocZAXw2Av8xYNZh6j6m\n",
       "kRcbDKCnN42I2KSG+KwJ5E2RJgKfBb4L/CiCheVGZWatoEj31PWkpHEhKXF8Mp8/g54TiTUxiT1I\n",
       "C1BOBzaN6HmipJlZT4rM07gvIjbrdm5mRGze0Mhq5O6p3kmMBk4jVb4dGcGUciMys2bQiO1eJWmH\n",
       "qgfb4xZGy5BYTuI40rphdwMbO2GYWX8V6Z46BDgvr3AL8Dzw6caFZPUisTPwE+AxYKsIniw5JDNr\n",
       "cYWrp/IOexERLzQ2pP5x91SFxFrAD4APAkdFcGXJIZlZk6p795SkL0haGXgBOEXSvZI+OpAgrTEk\n",
       "lpb4AjAL+COwoROGmdVTkTGNQyLiRWBXYFXSmkQnNDQqq5nE9qSJmHsAO0TwtQj+t+SwzKzNFBnT\n",
       "6Gq2fAy4MCIeqGzdbWWTWJ20sOBHgS8Dv/JKtGbWKEVaGjMk3QjsDlyfu6rebGxY1heJIRKHAw+S\n",
       "ug43iOBiJwwza6Qi8zSWIi1S+EREPC/pncDIAe7eV3edNBAuMZY0uXIRaVOk+0sOycxaVMP302hW\n",
       "nZA0JIYD3wE+QVoG5PwIt/rMrP8aMbnPSiYhiQnAQ6RxqA0jOM8Jw8wG2xIHwiWtGxGeDFYyiQ+Q\n",
       "uqKGAntFML3kkMysg/XW0rgUQNLNgxSLVZEYKvF9YBppT/WtnTDMrGy9ldwuJelrwPqSvsTi601F\n",
       "RJzS2NA6U94U6ePAD0kJY6MIFpQalJlZ1lvSOADYG1gKGEZKGlH13epMYgxp2fJRwIQIbi05JDOz\n",
       "xRQpud09Iq4dpHj6rZWrpyRWAI4FjiTNtj/NmyKZ2WBoRPXUHZJ+KGlG/vpB1Yq3NkASuwMPABsC\n",
       "m0VwshOGmTWrIi2Ny4DZwPmkrqkJwCYR8fHGh1dcq7U0JNYBTgU2Bj4XwQ0lh2RmHajuk/sk3R8R\n",
       "m/Z1rmytkjQklgW+CBwDnA6cFMFr5UZlZp2q1s/OIgsWvirpQxFxW77BDuDVU/tDYifSpkhPAttE\n",
       "8ETJIZmZ1aRI0vhP4IKqcYx/AAc1LqT2IzECOBnYAfgCcIUXFjSzVtRn0oiI+4BNupJGs+7c14wk\n",
       "lgaOAL4OnE1a/uOVcqMyM+u/Ii0NwMmiVhLbAj8l7am+YwRzSg7JzGzACicNK0binaS5Fh8DvgJM\n",
       "dleUmbULr3JbRxKfAuaQCgU2iOAiJwwzayd9tjQkLU36q3l01fVee6obiY8D3wN2i2Bm2fGYmTVC\n",
       "ke6pq4BXSRP8vH9DDyQ2BX4G/KsThpm1syJJY2REbNLwSFqUxLuA3wJHRXBP2fGYmTVSkTGNGyV9\n",
       "tOGRtKA8u/tS4JcRTC47HjOzRivS0rgDuFzSEHhrIb2IiJUbF1bzy/te/AT4O2kehplZ2yvS0jgF\n",
       "2BZYMSKG5a8BJwxJcyXNkjRT0vR8blVJUyQ9KulGScOrrp8o6TFJD0vadaD3r4PPkX4vE7xXt5l1\n",
       "iiJJ40/AgxFR7w/GAMZFxOYRsXU+dywwJSLWB6bmx0jaENiftHz4bsAZueVTColdgK8B4yN4qaw4\n",
       "zMwGW5HuqSeBWyRdB7yez9Wr5Lb7yorjgR3z8fmk7U6PBfYCJkfEQmCupMeBrYE76xBDTfLuer8E\n",
       "9o/gycG+v5lZmYr8tf4kcDOwLDCUtPXrsDrcO4CbJN0j6bB8bo2I6NoPewGwRj5eC3im6rXPACPr\n",
       "EENNJN4BXAkcH8G0wb6/mVnZiixYOKlB994+IuZLWh2YIunhbvcNSb3Npn7bc5ImVT2cFhHT6hIp\n",
       "ILEUcBFwcwRn1ut9zcwGk6RxwLj+vr7IjPBbejgdEfGR/t40v8H8/P0vki4ndTctkLRmRDwraQTw\n",
       "XL58HjCq6uVr53Pd33PSQGLqw/eAFUhLm5uZtaT8x/S0rseSjq/l9UXGNI6pOl4e+DdgUS036U7S\n",
       "isBSEfGSpJWAXYFvkrp+DgJOzN+vyC+5ErhI0imkbqkxwPSBxFBbvBxI+rm39v7dZtbJinRPdZ/l\n",
       "fLukuwd43zVIcz+6YvhlRNwo6R7gEkmHAnOB/XIMcyRdQloMcBFwRPS1T22d5CXOTwZ2iuBvg3FP\n",
       "M7NmVWSP8FWrHg4BtgROi4j3NTKwWjVij3CJtUkVWv8ZwdX1fG8zs2bQiD3C76Uy6LyI1AI4tPbQ\n",
       "WovECqTusR85YZiZJX22NFpFPVsaeYmQi4A3SDO+2+OXZGbWTa2fnX3O05C0r6Rh+fjrki6TtMVA\n",
       "gmwBE4H1gMOcMMzMKopM7vtGrnLaAdgZOBfad56CxF7AEcDeEbxadjxmZs2kSNJ4I3/fAzgrIq4G\n",
       "lmlcSOWR2Bg4C/h4BH8uOx4zs2ZTJGnMk/Rz0oKB10havuDrWorEaqTNlL4QMXhzQMzMWkmRktuV\n",
       "SCvLzoqIx/JM7Y0j4sbBCLCogQ6ES/wamBux2GRGM7O2Vutnp6unAIntgIuB93kcw8w6Sd2rp9pd\n",
       "Lq89GfgvJwwzs951fNIgrSm1AvA/ZQdiZtbsiswIb1sSywInAId7y1Yzs751ekvjs8AjEUwtOxAz\n",
       "s1bQsQPhEsOBR0mr1z7YuMjMzJqXB8KLOw74rROGmVlxHdnSkBgNzAA2imB+QwMzM2tibmkU89+k\n",
       "Jc+dMMzMatBxLQ2JLUnbx64fwcuNj8zMrHm5pdGLqol8xzthmJnVrqOSBrAnsBpwXtmBmJm1oo6Z\n",
       "3CexDHAS8KUIFpUdj5lZK+qklsZngHnAdWUHYmbWqjpiIFxiZeARYPcIZg5uZGZmzcsD4T37/8AU\n",
       "Jwwzs4Fp+5aGxEhgFrB5BH8a/MjMzJqXWxpv923g504YZmYD19bVUxKbAh8D1i87FjOzdtDuLY2T\n",
       "gG9H8ELZgZiZtYO2TRoSHwXWBX5WdixmZu2iLZOGxFLA94GvRrCw7HjMzNpFWyYN4CDgReCKsgMx\n",
       "M2snbVdym5cL+SOwbwR3lh2XmVkzc8kt7Ao87YRhZlZ/7Zg0DgQuKDsIM7N21FbdUxCrAnOBdSP4\n",
       "e8khmZk1vU7vntqXtMaUE4aZWQO0TNKQtJukhyU9JumrS7jsQOD8wYzLzKyTtETSkLQU8GNgN2BD\n",
       "4N8lbdDDpesD1w9mbM1I0riyY2gW/l1U+HdR4d9F/7VE0gC2Bh6PiLkRsRC4GNirh+smezIfAOPK\n",
       "DqCJjCs7gCYyruwAmsi4sgNoVa2SNEYCT1c9fiaf685VU2ZmDdQqSaNoide9DY3CzKzDtUTJraRt\n",
       "gUkRsVt+PBF4MyJOrLqm+X8QM7MmVEvJbaskjaVJe3zvDPwZmA78e0Q8VGpgZmYdpiU2YYqIRZI+\n",
       "B9wALAUu70uDAAAH10lEQVSc44RhZjb4WqKlYWZmzaFVBsJ7VXDiX9uTNErSLZIelPSApKPKjqlM\n",
       "kpaSNFPSVWXHUjZJwyVdKukhSXPyOGFHkjQx/z8yW9JFkpYrO6bBIulcSQskza46t6qkKZIelXSj\n",
       "pOG9vUfLJ40aJv51goXAFyPiA8C2wJEd/LsAOBqYQ/Hqu3Z2GnBtRGwAbAJ0ZPeupNHAYcAWEbEx\n",
       "qbv7gDJjGmTnkT4rqx0LTImI9YGp+fEStXzSoPjEv7YXEc9GxH35+GXSB8Na5UZVDklrA7sDZwOF\n",
       "K0PakaR3AB+KiHMhjRFGxAslh1WWF0l/XK2YC2xWBOaVG9LgiYjbgH90Oz2eyvJL5wN79/Ye7ZA0\n",
       "ik786yj5L6rNgbvKjaQ0PwSOAd4sO5AmsC7wF0nnSbpX0lmSViw7qDJExN+BHwB/IlViPh8RN5Ub\n",
       "VenWiIgF+XgBsEZvF7dD0nDXQzeShgKXAkfnFkdHkbQH8FxEzKTDWxnZ0sAWwBkRsQXwCn10QbQr\n",
       "SesBXwBGk1rhQyV9stSgmkikyqheP1PbIWnMA0ZVPR5Fam10JEnLAL8B/iciOnWP9O2A8ZKeBCYD\n",
       "H5HUyUvMPAM8ExF358eXkpJIJ9oSuCMi/hYRi4DLSP9eOtkCSWsCSBoBPNfbxe2QNO4BxkgaLWlZ\n",
       "YH/gypJjKoUkAecAcyLi1LLjKUtEHBcRoyJiXdIg580RcWDZcZUlIp4Fnpa0fj61C/BgiSGV6WFg\n",
       "W0kr5P9fdiEVS3SyK4GD8vFBQK9/bLbE5L7eeOLfYrYHPgXMkjQzn5sYEZ2+XLy7MOHzwC/zH1ZP\n",
       "AJ8uOZ5SRMT9udV5D2m8617g5+VGNXgkTQZ2BFaT9DTwDeAE4BJJh5J2Pt2v1/fw5D4zMyuqHbqn\n",
       "zMxskDhpmJlZYU4aZmZWmJOGmZkV5qRhZmaFOWmYmVlhThrWMJLWlHSxpMcl3SPpGkljBvH+42pd\n",
       "Fl3SWpJ+3Y97vUPSZwf6Pq1E0o6SPlh2HDa4nDSsIfJs28tJs7HfGxFbAhPpZTE0ZYMVYw/3Xzoi\n",
       "/hwR+/bj5asAR3Q9GMD71FVeybVRdqLGJTgaHI8NAicNa5SdgNcj4q3ZthExKyJur74oL//yiKTz\n",
       "gdnAKEnHSJou6X5Jk6qu/XrebOu2vHnOl/P5aZLG5uPV8ppTdLvP1pLuyKu8/r5rSQ1JB0u6UtJU\n",
       "YIqkd3dtUCPp7LyJ00xJz+X7ryTpJkkzJM2SND7f4gRgvXztifl9Hsjvs3xeYXZWvv+4qntfJum6\n",
       "vAHOiT39IiXNze85S9JdedE9JO0p6c78nlMkvSufnyTpQkm3A+fnWH6XY57R1TrILbFbJV0h6QlJ\n",
       "J0iakH/3syS9J1+3utIGTtPz13aS3g0cDnwx/8zb93RdT/EU++djTSsi/OWvun8BRwGnFLhuNPAG\n",
       "sHV+vCvws3w8BLgK+BCwFTATWBYYCjwKfClfdwtpUx2A1YAn8/E44Kp8PAxYKh/vAlyajw8mLa0/\n",
       "vCqe2d1ifDdpraZRpKVqhlXd67Gqa2Z3+7lm5+MvA2fn4/cBTwHL5Xs/kWNbjrSEw8gefkdPkpaD\n",
       "AZhQ9TMNr7rmM8DJ+XgScDewXH68QtXxGODuqt/PP0itv2VJi39Oqvrv98N8fBGwfT5eh7S2GcDx\n",
       "Xf8N+rhusXj81dpfbipao9SyPs1TETE9H+8K7Fq1dtZKpA+6YcAVEfE68HqtYxXAcOACSe/NsVX/\n",
       "278xIp7v6UWSlgd+DXw+Ip5WWkX4e5I+RFq7aK38F35v3WrbA6cDRMQjkp4C1s9xTI2Il/K95pCS\n",
       "TU+bAk3O3y8m7RUCqVV2CbAm6UP/j/l8AFdGxD/z42WBH0valJSgq8eV7o68l4Kkx0lruAE8QGot\n",
       "QkqyG1T1HA6TtFLXr6jqvZZ0Xfd4rIU5aVijPAh8ouC1r3R7/L2o6tYCkHQ0i39AVR8votLVuvwS\n",
       "7vFt0gf0PrlrZVrVc//bS2xnklolN+fHnyS1MLaIiDdyV9iS7rnYj7CE89UfpG+QWjJ96UrIPyK1\n",
       "Lq6WtCPpL/ou1T/TF4H5ETFBaXvk15Zw/zerHr9J5fNBwDY5Yb+lh+Gn3q7r7XdsLcRjGtYQ+UN2\n",
       "OUmHdZ2TtImkHfp46Q3AIV1/yUoaKWl14PfAnpKWU9pk6mNVr5lL2icBlpyoVibt1AYFV3iVdCQw\n",
       "NCJO6vY+z+WEsROpWwrgJVJrqCe3kZINeSxlHdIS3T0lkiUll/2rvt9RFUvXz3RwL++xMvBsPj6Q\n",
       "Yomp2o2k7qr05tJm+bD7z9z9uk1rvI+1ACcNa6R9gF2USm4fAP4bmN/DdW91ZUXEFFLf+B8kzQIu\n",
       "IX1w30Na938WcC1p0Lxrn+uTgc9Kuhd4J4t3jXUdn0TqVrqX9KEZVc9370rrevxlYKOqwfD/AH4J\n",
       "bJljm0Dah52I+Bvwe0mz84B29fueAQzJr7kYOCjSfva93bu7VSTdT1ri/Iv53CTg15LuAf7Sy890\n",
       "BnCQpPtIYyrVuzku6X7V73FU/pnvl/Qg8B/5/FXAPl0D4T1cd3iB+1iL8dLo1jIkrRQRryjtb30r\n",
       "cFhE3Fd2XI2Wu8DGRtrf2qxUHtOwVvJzSRuSxhB+0QkJI/NfdtY03NIwM7PCPKZhZmaFOWmYmVlh\n",
       "ThpmZlaYk4aZmRXmpGFmZoU5aZiZWWH/B8JJ6v6KmY+gAAAAAElFTkSuQmCC\n"
      ],
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10b19eb90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "logistic_regression_hyperparameters = {'C': [0.0001, 0.001, 0.01, 0.1, 0.5, 1.0, 5.0, 10.0]}\n",
    "logistic_regression_Cs = logistic_regression_hyperparameters['C']\n",
    "\n",
    "def get_logistic_regression_coefs(train_data, train_labels, C):\n",
    "    logistic_regression = LogisticRegression(C = C)\n",
    "    logistic_regression.fit(train_data, train_labels)\n",
    "    return logistic_regression.coef_\n",
    "\n",
    "sum_of_coef_squares = [\n",
    "    np.sum(get_logistic_regression_coefs(train_vectorized, train_labels, C) ** 2)\n",
    "    for C in logistic_regression_Cs\n",
    "]\n",
    "\n",
    "print '***** (3) b. Naive Bayes and Logistic Regression *****'\n",
    "print ''\n",
    "plt.plot(logistic_regression_Cs, sum_of_coef_squares)\n",
    "plt.xlabel('C regularization parameter')\n",
    "plt.ylabel('sum of squares for coefficients')\n",
    "_ = plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logistic_regression = LogisticRegression(C = 0.5)\n",
    "logistic_regression.fit(train_vectorized, train_labels)\n",
    "\n",
    "logistic_regression.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logistic_regression = LogisticRegression(C = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.5, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, penalty='l2', random_state=None, tol=0.0001)"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_regression.fit(train_vectorized, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -9.05577031e-02,   2.81899821e-02,  -8.81065249e-06, ...,\n",
       "         -1.18618257e-04,  -2.37236515e-04,  -8.39380776e-06],\n",
       "       [  1.25339447e-01,   3.31150400e-02,  -4.89836837e-03, ...,\n",
       "         -9.60600061e-04,  -1.92120012e-03,   1.42648095e-02],\n",
       "       [ -5.93499619e-02,  -1.30402995e-01,   2.91311854e-03, ...,\n",
       "          1.23729749e-03,   2.47459497e-03,  -1.08694131e-02],\n",
       "       [ -8.05802508e-02,  -8.61766684e-03,  -6.86940659e-03, ...,\n",
       "         -1.58798370e-04,  -3.17596740e-04,  -6.88253702e-06]])"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_regression.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** (4) selecting logistic features *****\n",
      "\n",
      "  Top 5 features by category:\n",
      "   Category alt.atheism: [u'atheism', u'atheists', u'bobby', u'deletion', u'religion']\n",
      "   Category comp.graphics: [u'3d', u'computer', u'file', u'graphics', u'image']\n",
      "   Category sci.space: [u'launch', u'nasa', u'orbit', u'space', u'spacecraft']\n",
      "   Category talk.religion.misc: [u'blood', u'christian', u'christians', u'fbi', u'order']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "N = 5\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "def select_top_n_by_weight_indices(coefs, N):\n",
    "    return np.array(coefs).argsort()[(-N):]\n",
    "\n",
    "print '***** (4) selecting logistic features *****'\n",
    "print ''\n",
    "print '  Top 5 features by category:'\n",
    "\n",
    "for index, category in enumerate(newsgroups_train.target_names):\n",
    "    coefs = logistic_regression.coef_[index]\n",
    "    top_n_indices = select_top_n_by_weight_indices(coefs, N)\n",
    "    \n",
    "    features = select_features(feature_names, top_n_indices)\n",
    "    \n",
    "    print '   Category ' + category + ': ' + str(features)\n",
    "    \n",
    "print ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1145  3866  3870  4743  4784  5901  5904  6555  7841 10234 10376 11552\n",
      " 12769 14540 16697 17597 17609 20430 22567 22570]\n",
      "[u'3d', u'atheism', u'atheists', u'blood', u'bobby', u'christian', u'christians', u'computer', u'deletion', u'fbi', u'file', u'graphics', u'image', u'launch', u'nasa', u'orbit', u'order', u'religion', u'space', u'spacecraft']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "top_feature_indices_by_category = [\n",
    "    select_top_n_by_weight_indices(logistic_regression.coef_[category_index], N)\n",
    "    for category_index \n",
    "    in range(len(newsgroups_train.target_names))   \n",
    "]\n",
    "\n",
    "top_feature_indices = np.sort(np.reshape(top_feature_indices_by_category, -1))\n",
    "top_feature_names = select_features_by_indices(feature_names, top_feature_indices)\n",
    "print(top_feature_indices)\n",
    "print(top_feature_names)\n",
    "\n",
    "weights_by_category = {\n",
    "    category : logistic_regression.coef_[index][top_feature_indices]\n",
    "    for index, category \n",
    "    in enumerate(newsgroups_train.target_names)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_frame = pd.DataFrame(weights_by_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_feature_indices = np.reshape(top_feature_indices, -1).sort()\n",
    "top_feature_indices\n",
    "#select_features(feature_names, top_feature_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alt.atheism</th>\n",
       "      <th>comp.graphics</th>\n",
       "      <th>sci.space</th>\n",
       "      <th>talk.religion.misc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3d</th>\n",
       "      <td>0.723998</td>\n",
       "      <td>-0.249342</td>\n",
       "      <td>-0.272409</td>\n",
       "      <td>-0.270038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>atheism</th>\n",
       "      <td>0.794833</td>\n",
       "      <td>-0.351204</td>\n",
       "      <td>-0.367056</td>\n",
       "      <td>-0.377134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>atheists</th>\n",
       "      <td>0.798343</td>\n",
       "      <td>-0.508539</td>\n",
       "      <td>-0.658884</td>\n",
       "      <td>-0.050760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blood</th>\n",
       "      <td>0.823577</td>\n",
       "      <td>-0.191297</td>\n",
       "      <td>-0.286486</td>\n",
       "      <td>-0.387305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bobby</th>\n",
       "      <td>0.834500</td>\n",
       "      <td>-0.093333</td>\n",
       "      <td>-0.266974</td>\n",
       "      <td>-0.637608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>christian</th>\n",
       "      <td>0.071378</td>\n",
       "      <td>0.844623</td>\n",
       "      <td>-0.569541</td>\n",
       "      <td>-0.396300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>christians</th>\n",
       "      <td>-0.303420</td>\n",
       "      <td>0.941862</td>\n",
       "      <td>-0.571115</td>\n",
       "      <td>-0.317585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>computer</th>\n",
       "      <td>-0.275313</td>\n",
       "      <td>1.059663</td>\n",
       "      <td>-0.682605</td>\n",
       "      <td>-0.506389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deletion</th>\n",
       "      <td>-0.469545</td>\n",
       "      <td>1.116957</td>\n",
       "      <td>-0.671895</td>\n",
       "      <td>-0.380328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fbi</th>\n",
       "      <td>-0.640874</td>\n",
       "      <td>1.626252</td>\n",
       "      <td>-1.103020</td>\n",
       "      <td>-0.626396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>file</th>\n",
       "      <td>-0.297492</td>\n",
       "      <td>-0.322145</td>\n",
       "      <td>0.737287</td>\n",
       "      <td>-0.291839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>graphics</th>\n",
       "      <td>-0.377756</td>\n",
       "      <td>-0.396782</td>\n",
       "      <td>0.783895</td>\n",
       "      <td>-0.280636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>image</th>\n",
       "      <td>-0.465515</td>\n",
       "      <td>-0.410416</td>\n",
       "      <td>0.851823</td>\n",
       "      <td>-0.401650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>launch</th>\n",
       "      <td>-0.350330</td>\n",
       "      <td>-0.559685</td>\n",
       "      <td>1.012819</td>\n",
       "      <td>-0.499053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nasa</th>\n",
       "      <td>-1.061504</td>\n",
       "      <td>-1.116200</td>\n",
       "      <td>1.863981</td>\n",
       "      <td>-0.972067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>orbit</th>\n",
       "      <td>-0.649534</td>\n",
       "      <td>-0.065685</td>\n",
       "      <td>-0.124164</td>\n",
       "      <td>0.742944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>order</th>\n",
       "      <td>-0.244043</td>\n",
       "      <td>-0.217528</td>\n",
       "      <td>-0.374966</td>\n",
       "      <td>0.754670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>religion</th>\n",
       "      <td>-0.428838</td>\n",
       "      <td>-0.096932</td>\n",
       "      <td>-0.230684</td>\n",
       "      <td>0.847014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>space</th>\n",
       "      <td>-0.482941</td>\n",
       "      <td>-0.339080</td>\n",
       "      <td>-0.247553</td>\n",
       "      <td>0.925647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spacecraft</th>\n",
       "      <td>-0.607427</td>\n",
       "      <td>-0.315386</td>\n",
       "      <td>-0.404441</td>\n",
       "      <td>0.929541</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            alt.atheism  comp.graphics  sci.space  talk.religion.misc\n",
       "3d             0.723998      -0.249342  -0.272409           -0.270038\n",
       "atheism        0.794833      -0.351204  -0.367056           -0.377134\n",
       "atheists       0.798343      -0.508539  -0.658884           -0.050760\n",
       "blood          0.823577      -0.191297  -0.286486           -0.387305\n",
       "bobby          0.834500      -0.093333  -0.266974           -0.637608\n",
       "christian      0.071378       0.844623  -0.569541           -0.396300\n",
       "christians    -0.303420       0.941862  -0.571115           -0.317585\n",
       "computer      -0.275313       1.059663  -0.682605           -0.506389\n",
       "deletion      -0.469545       1.116957  -0.671895           -0.380328\n",
       "fbi           -0.640874       1.626252  -1.103020           -0.626396\n",
       "file          -0.297492      -0.322145   0.737287           -0.291839\n",
       "graphics      -0.377756      -0.396782   0.783895           -0.280636\n",
       "image         -0.465515      -0.410416   0.851823           -0.401650\n",
       "launch        -0.350330      -0.559685   1.012819           -0.499053\n",
       "nasa          -1.061504      -1.116200   1.863981           -0.972067\n",
       "orbit         -0.649534      -0.065685  -0.124164            0.742944\n",
       "order         -0.244043      -0.217528  -0.374966            0.754670\n",
       "religion      -0.428838      -0.096932  -0.230684            0.847014\n",
       "space         -0.482941      -0.339080  -0.247553            0.925647\n",
       "spacecraft    -0.607427      -0.315386  -0.404441            0.929541"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def vectorize_with_word_bigrams(data):\n",
    "    vectorizer = CountVectorizer(ngram_range = (2, 2))\n",
    "    vectorizer.fit(data)\n",
    "    return vectorizer\n",
    "\n",
    "bigram_vectorizer = vectorize_with_word_bigrams(train_data)\n",
    "train_bigram_vectorized = bigram_vectorizer.transform(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2034, 194891)"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(train_bigram_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.5, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, penalty='l2', random_state=None, tol=0.0001)"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    bigram_vectorizer = vectorize_with_word_bigrams(train_data)\n",
    "    train_bigram_vectorized = bigram_vectorizer.transform(train_data)\n",
    "    bigram_logistic_regression = LogisticRegression(C = 0.5)\n",
    "    bigram_logistic_regression.fit(train_bigram_vectorized, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = bigram_vectorizer\n",
    "train_vectorized = train_bigram_vectorized\n",
    "logistic_regression = bigram_logistic_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_names = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_feature_indices_by_category = [\n",
    "            select_top_n_by_weight_indices(logistic_regression.coef_[category_index], N)\n",
    "            for category_index \n",
    "            in range(len(newsgroups_train.target_names))   \n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 20596, 193317, 184771,  37174,  38326]),\n",
       " array([ 88596, 123709,  40451,  81950,  98723]),\n",
       " array([ 90070,  16732, 145068, 165984, 167336]),\n",
       " array([116318,  32640,  80535,  37174, 164806])]"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_feature_indices_by_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_feature_indices = np.unique(np.sort(np.reshape(top_feature_indices_by_category, -1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 16732,  20596,  32640,  37174,  38326,  40451,  80535,  81950,\n",
       "        88596,  90070,  98723, 116318, 123709, 145068, 164806, 165984,\n",
       "       167336, 184771, 193317])"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_feature_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_feature_names = select_features_by_indices(feature_names, top_feature_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'and such',\n",
       " u'are you',\n",
       " u'but he',\n",
       " u'cheers kent',\n",
       " u'claim that',\n",
       " u'comp graphics',\n",
       " u'ignorance is',\n",
       " u'in advance',\n",
       " u'is there',\n",
       " u'it was',\n",
       " u'looking for',\n",
       " u'of jesus',\n",
       " u'out there',\n",
       " u'sci space',\n",
       " u'the fbi',\n",
       " u'the moon',\n",
       " u'the space',\n",
       " u'was just',\n",
       " u'you are']"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 16732,  20596,  32640,  37174,  37174,  38326,  40451,  80535,\n",
       "        81950,  88596,  90070,  98723, 116318, 123709, 145068, 164806,\n",
       "       165984, 167336, 184771, 193317])"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort(np.reshape(top_feature_indices_by_category, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'cheers kent'"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names[37174]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def empty_preprocessor(s):\n",
    "    return s\n",
    "\n",
    "def vectorize_with_preprocessor(data, preprocessor = empty_preprocessor):\n",
    "    vectorizer = CountVectorizer(preprocessor = preprocessor)\n",
    "    vectorizer.fit(data)\n",
    "    return vectorizer\n",
    "\n",
    "def f_score_with_preprocessor(train_data, train_labels, dev_data, dev_labels, \n",
    "                              preprocessor = empty_preprocessor):\n",
    "    \n",
    "    \n",
    "    vectorizer = vectorize_with_preprocessor(train_data, preprocessor)\n",
    "    train_vectorized = vectorizer.transform(train_data)\n",
    "    \n",
    "    logistic_regression = LogisticRegression(C = 0.5)\n",
    "    logistic_regression.fit(train_vectorized, train_labels)\n",
    "    \n",
    "    dev_vectorized = vectorizer.transform(dev_data)\n",
    "    dev_predictions = logistic_regression.predict(dev_vectorized)\n",
    "    \n",
    "    f_score = metrics.f1_score(dev_labels, dev_predictions)\n",
    "    \n",
    "    return f_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.70691091854935018"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_score_with_preprocessor(\n",
    "    train_data, train_labels, dev_data, dev_labels,\n",
    "    empty_preprocessor)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "baseline_vectorizer = vectorize_with_preprocessor(train_data)\n",
    "baseline_train_vectorized = baseline_vectorizer.transform(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.5, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, penalty='l2', random_state=None, tol=0.0001)"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_logistic_regression = LogisticRegression(C = 0.5)\n",
    "baseline_logistic_regression.fit(baseline_train_vectorized, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline f1 score: 0.708473977649\n"
     ]
    }
   ],
   "source": [
    "baseline_dev_vectorized = baseline_vectorizer.transform(dev_data)\n",
    "baseline_dev_predictions = baseline_logistic_regression.predict(baseline_dev_vectorized)\n",
    "baseline_f1_score = metrics.f1_score(dev_labels, baseline_dev_predictions)\n",
    "\n",
    "print 'Baseline f1 score: ' + str(baseline_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u\"Hi,\\n\\nI've noticed that if you only save a model (with all your mapping planes\\npositioned carefully) to a .3DS file that when you reload it after restarting\\n3DS, they are given a default position and orientation.  But if you save\\nto a .PRJ file their positions/orientation are preserved.  Does anyone\\nknow why this information is not stored in the .3DS file?  Nothing is\\nexplicitly said in the manual about saving texture rules in the .PRJ file. \\nI'd like to be able to read the texture rule information, does anyone have \\nthe format for the .PRJ file?\\n\\nIs the .CEL file format available from somewhere?\\n\\nRych\",\n",
       " u'\\n\\nSeems to be, barring evidence to the contrary, that Koresh was simply\\nanother deranged fanatic who thought it neccessary to take a whole bunch of\\nfolks with him, children and all, to satisfy his delusional mania. Jim\\nJones, circa 1993.\\n\\n\\nNope - fruitcakes like Koresh have been demonstrating such evil corruption\\nfor centuries.',\n",
       " u\"\\n >In article <1993Apr19.020359.26996@sq.sq.com>, msb@sq.sq.com (Mark Brader) \\n\\nMB>                                                             So the\\nMB> 1970 figure seems unlikely to actually be anything but a perijove.\\n\\nJG>Sorry, _perijoves_...I'm not used to talking this language.\\n\\nCouldn't we just say periapsis or apoapsis?\\n\\n \",\n",
       " u'I have a request for those who would like to see Charley Wingate\\nrespond to the \"Charley Challenges\" (and judging from my e-mail, there\\nappear to be quite a few of you.)  \\n\\nIt is clear that Mr. Wingate intends to continue to post tangential or\\nunrelated articles while ingoring the Challenges themselves.  Between\\nthe last two re-postings of the Challenges, I noted perhaps a dozen or\\nmore posts by Mr. Wingate, none of which answered a single Challenge.  \\n\\nIt seems unmistakable to me that Mr. Wingate hopes that the questions\\nwill just go away, and he is doing his level best to change the\\nsubject.  Given that this seems a rather common net.theist tactic, I\\nwould like to suggest that we impress upon him our desire for answers,\\nin the following manner:\\n\\n1. Ignore any future articles by Mr. Wingate that do not address the\\nChallenges, until he answers them or explictly announces that he\\nrefuses to do so.\\n\\n--or--\\n\\n2. If you must respond to one of his articles, include within it\\nsomething similar to the following:\\n\\n    \"Please answer the questions posed to you in the Charley Challenges.\"\\n\\nReally, I\\'m not looking to humiliate anyone here, I just want some\\nhonest answers.  You wouldn\\'t think that honesty would be too much to\\nask from a devout Christian, would you?  \\n\\nNevermind, that was a rhetorical question.',\n",
       " u'AW&ST  had a brief blurb on a Manned Lunar Exploration confernce\\nMay 7th  at Crystal City Virginia, under the auspices of AIAA.\\n\\nDoes anyone know more about this?  How much, to attend????\\n\\nAnyone want to go?']"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u\"hi i've noticed that if you only save a model with all your mapping planes positioned carefully to a ds file that when you reload it after restarting ds y are given a default position and orientation but if you save to a prj file ir positions orientation are preserved does anyone know why this information is not stored in ds file nothing is explicitly said in manual about saving texture rules in prj file i'd like to be able to read texture rule information does anyone have format for prj file is cel file format available from somewhere rych\""
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocessor_v1(s):\n",
    "    s = s.lower() # lowercase the string\n",
    "    s = re.sub(r\"[^a-z']+\", \" \", s) # replace all non-alpha/apostrophe characters with a single space\n",
    "    s = re.sub(r\"(^|\\s)'+\", \" \", s) # replace leading apostrophes with a single space\n",
    "    s = re.sub(r\"'+($|\\s)\", \" \", s) # replace trailing apostrophes with a single space\n",
    "    s = re.sub(\"the\", \" \", s) # throw out all the's\n",
    "    s = re.sub(r\"[\\s]+\", \" \", s) # replace all white space with single space \n",
    "    return s\n",
    "preprocessor = preprocessor_v1\n",
    "\n",
    "preprocessor(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"here's a quote something something else foo \""
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor(\"Here's a quote 'something' 'something else' ''foo''.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.70691091854935018"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#baseline\n",
    "f_score_with_preprocessor(\n",
    "    train_data, train_labels, dev_data, dev_labels,\n",
    "    empty_preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.722153661284\n",
      "[u' ctully thism bsd ignrnc ignrnc th xistnc ny dn fll int thists t bliv bcus thir prid mistk buchin bbb vic tk thy sid tht quns culd sty thy blw brnx wy snk mnhttn t ', u' might sur yu wuld ls wrng ', u' pckg bsd svrl rticls but stndrd rdisity sm unpublishd mthds min rticls chn chn wllc grnbrg prgrssiv rfinmnt pprch fst rdisity img gnrtin cmputr grphics siggrph N N N ugust N silin puch gnrl pss mthd intgrting spculr diffus rflctin cmputr grphics siggrph n ppN july N d us hmi cubs hv spcil hrdwr sprcsttin nnymus usrnm yur mil ddrss psswrd stphn stphn mnn cmputr grphics univrsity brn switzrlnd mnn unib tl N N fx N N prjcts rdisity rytrcing cmputr grphics ', u'srry missd rymnd ws just in dhlgrn lst mnth m virtul rlity mrkt mngr silicn grphics prhps cn hlp littl unfrtuntly whil systms wr usd crt spcil ffcts bth trmintr nd lwnmwr ths film qulity cmputr grphics rndrd sftwr writtn film frm  tim ch frm cmputr nimtin ths films tk hurs rndr high prlll prcssing cmputr systms thus tht lvl grphics wuld difficult nt impssibl chiv rl tim frms scnd dpnds upn srius r hw dvncd yur pplictin tru immrsiv visuliztin rquirs rndring cmplx visul dtbss nywhr frm t nwly rndrd frms scnd this  similr rquirmnt tht trditinl flight simultrs pilt trining th frm rt t th usr ntics stpping th frms thy mv thir hd rpidly rund scn th mtin th grphics nt smth cntiguus thus grphics systm must pwrful nugh sustin high frm rts whil rndring cmplx dt rprsnttins dditinlly frm rt must cnstnt th systm rndrs frms scnd n pint thn frms scnd nxt prhps t scn th viwing dirctin bing simplr thn wht visibl bfr usr gt hvily distrctd th mdium grphics cmputr rthr thn fcusing th dt mintin cnstnt frm rt systm must bl run rl tim unix gnrl ds supprt rl tim prtin silicn grphics mdifid unix krnl its multi prcssr systms b bl supprt rl tim prtin bypssing usul unix prcss pririty mngmnt schms uniprcssr systms running unix cnnt fundmntlly supprt rl tim prtin sun sprcN hp sris systms ibm N vn s uniprcssr systms lik indig crimsn nly multiprcssr nyx chllng systms supprt rl tim prtin t thir symmtric multi prcssing shrd mmry rchitctur frm grphics prspctiv rndring cmplx virtul nvirnmnts rquirs dvncd rndring tchniqus lik txtur mpping rl tim multi smpl nti lising ll th gnrl purps grphics systms th mrkt tdy nly crimsn rlityngin nyx rlitynginN systms fully supprt ths cpbilitis nti lising prticulrly imprtnt th crwling jggd dgs lisd plygns n unfrtunt distrctin whn immrsd  virtul nvirnmnt cn th gnrl purps grphics librris listd bv dvlp pplictins tht strting  prtty lvl thr ff shlf sftwr pckgs vilbl gt ging much fstr bing trgtd dirctly th pplictin dvlpr sm th mst ppulr in prticulr rdr divisin rdwd city dvs snsN suslit wrldtlkit nvl pstgrdut schl mntry npsnt fr gmini tchnlgy crp irvin gvs simtin sris prdigm simultin dlls visinwrks udiwrks silicn grphics muntin viw iris prfrmr thr sm thrs nt th f hd thr t mny list hr hr  smttring fk spc lbs mnl prk bm virtul tchnlgis stnfrd cybrglv digitl img dsign yrk th crickt input kisr lctr ptics crlsbd sim hlmt displys virtul rsrch sunnyvl flight hlmt disply virtul rlity plsntvill hd displys w sftwr systms js Nd mdling sftwr tc rd sm th bks th mrkt virtul rlity pimntl kn txir virtul mirg rtificil rlity myrn krugr r chck th nwsgrup virtul wrlds fl fr cntct fr mr inf rgrds jsh ', u' yu frc t smthing i mrlly rspnsibl it wll mk yur mind it b instinctiv t murdr nt s vn crrct nimls th sm spcis kill nthr sigh wndr mny tims hv bn rund this lp think tht instinctiv bhviur n mrl significnc m quit prprd bliv tht highr nimls such primts hv bginnings  mrl sns sinc thy sm xhibit slf wrnss wht yu trying sy tht sm killing nimls  mrl significnc sm ds is this yur nturl mrlity yu blind wht yu think tht this sntnc mns thr must th pssibility tht rgnism s just ppl r tlking but cnsidr ltrntivs wht wuld tht imply find fct tht thy t significnt ', u'hr sm rcnt bsrvtins tkn th hubbl spc tlscp th fint bjct spctrgrph ws usd mk ultrvilt bsrvtins bth plnt plut its mn chrn pkups wr succssful bsrvtins wr xcutd schduld n prblms wr rprtd bsrvtins wr md using high spd phtmtr th plnt urnus during cculttin  fint str cpricrnus ths bsrvtins will hlp ur undrstnding th plnt tmsphric rditiv dynmicl prcsss this vnt ccurrd cls th lst qurtr mn spcil rrngmnts t md mdify lunr limit tsts llw ths bsrvtins bsrvtins currntly bing rviwd ll bsrvtins lkd ky ', u' guss tht why scintists prbbly rn mntind ithr stck brkrs tlvisin rpirmn s prcius knw just dp brinwshing frm childhd tht tks prgrss rligin clns wy vry substntil prt th rsning nurns dn mind i t xist ', u'hi cn smn pint twrds sm rticls bids flcking lgrithms ls rticls prticl nimtin frmuls wuld nic dlux N N hd c hi this  signtur virus c', u' sms didn undrstnd nything but rlitis lir luntic th rl thing  vry nrrw viw th pssibilitis jsus mssg sigh sms rligin mks yur mind brin filtr nything tht ds fit int yur prsnl schm nyn tht thinks pssibilitis with jsus bund th clssicl lwis ntin lir luntic sint indd bund bcm christin chrs knt', u' culd this pssibly nvirnmntl vndlism whn thr n nvirnmnt vndliz thr sinc dvrtising just hlp dfry csts s crtinly surpris tht txpyrs wuld br mst th xpns sunds lik gd id m sinc txpyrs wuld br f xpns thy didn d dvrtising grt id thy shuld hv dn lng i t bliv tht mil lng billbrd wuld hv significnt ffct th vrll brightnss vnus visibl during dy nbdy cmplins but tht bsids s l it wuld nly visibl during twilight whn sky lrdy bright vn it wuld hv sm miniscul impct wuld nly  shrt tim it gs zipping crss sky dubt ppl lwys lking smthing prtst but it wuld n surpris wll lk th bright sid imgin lks th fcs ppl primitiv tribs in middl nwhr thy lk nd  f budwisr flying crss sky ']\n"
     ]
    }
   ],
   "source": [
    "def preprocessor_v2(s):\n",
    "    s = s.lower() # lowercase the string\n",
    "    s = re.sub(r\"[^a-z0-9]+\", \" \", s) # replace all non-alphanumeric characters with a single space\n",
    "    s = re.sub(r\"[\\s]+\", \" \", s) # replace all white space with single space\n",
    "    s = re.sub(r\" [a-z0-9]{1,3} \", \" \", s) # throw out the short words between 1 and 3 characters\n",
    "    s = re.sub(r\"[0-9]+\", \"N\", s) # replace all numbers with N\n",
    "    s = re.sub(r\"[aeo]\", \"\", s) # discard all a, e, and o (i's turn out to be important)   \n",
    "    return s\n",
    "\n",
    "print(f_score_with_preprocessor(\n",
    "    train_data, train_labels, dev_data, dev_labels,\n",
    "    preprocessor = preprocessor_v2))\n",
    "\n",
    "print([\n",
    "        preprocessor_v2(s)\n",
    "        for s \n",
    "        in train_data[30:40]\n",
    "        ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " For example, try lowercasing everything, replacing sequences of numbers with a single token, removing various other non-letter characters, and shortening long words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u\"Hi,\\n\\nI've noticed that if you only save a model (with all your mapping planes\\npositioned carefully) to a .3DS file that when you reload it after restarting\\n3DS, they are given a default position and orientation.  But if you save\\nto a .PRJ file their positions/orientation are preserved.  Does anyone\\nknow why this information is not stored in the .3DS file?  Nothing is\\nexplicitly said in the manual about saving texture rules in the .PRJ file. \\nI'd like to be able to read the texture rule information, does anyone have \\nthe format for the .PRJ file?\\n\\nIs the .CEL file format available from somewhere?\\n\\nRych\""
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23905"
      ]
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = vectorize_with_preprocessor(train_data, preprocessor)\n",
    "len(vectorizer.get_feature_names())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
